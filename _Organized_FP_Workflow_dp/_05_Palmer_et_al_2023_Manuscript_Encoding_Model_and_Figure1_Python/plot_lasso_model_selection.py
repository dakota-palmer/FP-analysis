# -*- coding: utf-8 -*-
"""
Created on Wed Jan 26 15:27:18 2022

@author: Dakota
"""

"""
===================================================
Lasso model selection: Cross-Validation / AIC / BIC
===================================================

Use the Akaike information criterion (AIC), the Bayes Information
criterion (BIC) and cross-validation to select an optimal value
of the regularization parameter alpha of the :ref:`lasso` estimator.

Results obtained with LassoLarsIC are based on AIC/BIC criteria.

Information-criterion based model selection is very fast, but it
relies on a proper estimation of degrees of freedom, are
derived for large samples (asymptotic results) and assume the model
is correct, i.e. that the data are actually generated by this model.
They also tend to break when the problem is badly conditioned
(more features than samples).

For cross-validation, we use 20-fold with 2 algorithms to compute the
Lasso path: coordinate descent, as implemented by the LassoCV class, and
Lars (least angle regression) as implemented by the LassoLarsCV class.
Both algorithms give roughly the same results. They differ with regards
to their execution speed and sources of numerical errors.

Lars computes a path solution only for each kink in the path. As a
result, it is very efficient when there are only of few kinks, which is
the case if there are few features or samples. Also, it is able to
compute the full path without setting any meta parameter. On the
opposite, coordinate descent compute the path points on a pre-specified
grid (here we use the default). Thus it is more efficient if the number
of grid points is smaller than the number of kinks in the path. Such a
strategy can be interesting if the number of features is really large
and there are enough samples to select a large amount. In terms of
numerical errors, for heavily correlated variables, Lars will accumulate
more errors, while the coordinate descent algorithm will only sample the
path on a grid.

Note how the optimal value of alpha varies for each fold. This
illustrates why nested-cross validation is necessary when trying to
evaluate the performance of a method for which a parameter is chosen by
cross-validation: this choice of parameter may not be optimal for unseen
data.
"""

def plot_lasso_model_selection(X, y, cv, modelName, savePath):
    print(__doc__)
    
    # Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort
    # License: BSD 3 clause
    
    import time
    
    import numpy as np
    import matplotlib.pyplot as plt
    
    from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC
    from sklearn import datasets
    
    from customFunctions import saveFigCustom
    
    # This is to avoid division by zero while doing np.log10
    EPSILON = 1e-4
    
    # Xx, yY = datasets.load_diabetes(return_Xx_y=True)
    #changed from X, y just so can run in middle of encodingModel without redefining
    
    # Xx= group.loc[:,X].copy()
    # yY= group.loc[:,y].copy()
    
    Xx= X
    yY= y

    #REMOVE COLUMNS WITH ALL ZERO (e.g. UStime wil never occur with a late enough time shift, fixed time relative to DS)
    col= Xx.columns[Xx.sum(axis=0)==0]
    
    Xx= Xx.drop(col,axis=1)

    #maybe conversion from sparse to dense?
    #where are nans coming from?? not an issue until this script?
    # test= np.isnan(Xx) #gives some true values
    # test= pd.isnull(Xx)
        
    # #add bad features, necessary?
    rng = np.random.RandomState(42)
    Xx = np.c_[Xx, rng.randn(Xx.shape[0], 14)]  # add some bad features
    # Xx2 = np.c_[Xx, rng.randn(Xx.shape[0], 14)]  # add some bad features


    # normalize data as done by Lars to allow for comparison
    #--! Debugging notes This step adds nans for some reason? Likely comes from:
    # 93: RuntimeWarning: invalid value encountered in true_divide
    #guessing from zeros/binary coding?
    # test= np.sum(Xx ** 2, axis=0)
    # test2= np.sqrt(test)
    #sqrt of some of test ==0, thus as divisor would be leading to nan
    #issue is @ col 2361:2399
    #check original values
    #Correspond to late US shifts. Makes sense- these will never have timestamps past a certain point.
    #So, should just remove these columns!
    # test3= group.iloc[:,2361:2399]
    
    Xx /= np.sqrt(np.sum(Xx ** 2, axis=0))
    # Xx2 /= np.sqrt(np.sum(Xx2 ** 2, axis=0))
    
    #try just removing rows with nans?
    # Xx= Xx[~np.isnan(Xx).any(axis=1),:] #returns nothing, is a whole column nan?
    #80 columns w all nan?
    # test= Xx[:,np.isnan(Xx).any(axis=0)]
    # Xx= Xx[:,~np.isnan(Xx).any(axis=0)]
    
    #original df is clearly fine, none of these return any nans:
    # test= group.loc[:,X]
    # test2= test.loc[:,np.isnan(test).any(axis=0)]
    # test3= test.loc[np.isnan(test).any(axis=1),:]

    
    # #############################################################################
    # LassoLarsIC: least angle regression with BIC/AIC criterion
    
    model_bic = LassoLarsIC(criterion='bic')
    t1 = time.time()
    model_bic.fit(Xx, yY)
    ##?getting error here for some reason due to nan/inf values? idk where they come from
    # test= Xx[np.isnan(Xx)]
    # test= yY[np.isnan(yY)]
    # test= Xx[np.isinf(Xx)]
    # test= yY[np.isinf(yY)]

    
    t_bic = time.time() - t1
    alpha_bic_ = model_bic.alpha_
    
    model_aic = LassoLarsIC(criterion='aic')
    model_aic.fit(Xx, yY)
    alpha_aic_ = model_aic.alpha_
    
    def plot_ic_criterion(model, name, color):
        criterion_ = model.criterion_
        plt.semilogx(model.alphas_ + EPSILON, criterion_, color=color,
                      linewidth=3, label=('%s criterion= ' % name+str(model.alpha_)))
    
        plt.axvline(model.alpha_ + EPSILON, linestyle='--', color=color, linewidth=3,
                    label=('alpha: %s estimate='+str(model.alpha_)) % name)
        plt.xlabel(r'$\alpha$')
        plt.ylabel('criterion')
        
    def plot_ic_criterion_nonLog(model, name, color):
        criterion_ = model.criterion_
        plt.plot(model.alphas_ + EPSILON, criterion_, color=color,
             linewidth=3, label=('%s criterion= ' % name+str(model.alpha_)))
        plt.axvline(model.alpha_ + EPSILON, linestyle='--', color=color, linewidth=3,
                    label=('alpha: %s estimate='+str(model.alpha_)) % name)
        plt.xlabel(r'$\alpha$')
        plt.ylabel('criterion')
    
    
    # fig= plt.figure()
    # plot_ic_criterion(model_aic, 'AIC', 'b')
    # plot_ic_criterion(model_bic, 'BIC', 'r')
    # plt.legend()
    # plt.title('Information-criterion for model selection (training time %.3fs)'
    #           % t_bic)
    
   
    # # saveFigCustom(plt.gcf, modelName+'-modelSelection_AIC-BIC', savePath)
    
    
    # #############################################################################
    # LassoCV: coordinate descent
    
    # Compute paths
    print("Computing regularization path using the coordinate descent lasso...")
    t1 = time.time()
    model_cd = LassoCV(cv=cv).fit(Xx, yY)
    t_lasso_cv = time.time() - t1
    
    # # Display results
    # plt.figure()
    # # ymin, ymax = 2300, 3800
    # plt.semilogx(model_cd.alphas_ + EPSILON, model_cd.mse_path_, ':')
    # plt.plot(model_cd.alphas_ + EPSILON, model_cd.mse_path_.mean(axis=-1), 'k',
    #          label='Average across the folds', linewidth=2)
    # plt.axvline(model_cd.alpha_ + EPSILON, linestyle='--', color='k',
    #             label='alpha: CV estimate= '+str(model_cd.alpha_))
    
    # plt.legend()
    
    # plt.xlabel(r'$\alpha$')
    # plt.ylabel('Mean square error')
    # plt.title('Mean square error on each fold: coordinate descent '
    #           '(train time: %.2fs)' % t_lasso_cv)
    # plt.axis('tight')
    # # plt.ylim(ymin, ymax)
    
    # # saveFigCustom(plt.gcf, modelName+'-modelSelection_CV-coordinate-descent', savePath)
    
    
    # #############################################################################
    # LassoLarsCV: least angle regression
    
    # Compute paths
    print("Computing regularization path using the Lars lasso...")
    t1 = time.time()
    model_lars = LassoLarsCV(cv=cv).fit(Xx, yY)
    t_lasso_lars_cv = time.time() - t1
    
    # # Display results
    # plt.figure()
    # plt.semilogx(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_, ':')
    # plt.semilogx(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_.mean(axis=-1), 'k',
    #              label='Average across the folds', linewidth=2)
    # plt.axvline(model_lars.alpha_, linestyle='--', color='k',
    #             label='alpha CV= '+str(model_lars.alpha_))
    # plt.legend()
    
    # plt.xlabel(r'$\alpha$')
    # plt.ylabel('Mean square error')
    # plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
    #           % t_lasso_lars_cv)
    
    # plt.show()

    # plt.axis('tight')
    
    
    # plt.figure()
    # plt.plot(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_, ':')
    # plt.plot(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_.mean(axis=-1), 'k',
    #              label='Average across the folds', linewidth=2)
    # plt.axvline(model_lars.alpha_, linestyle='--', color='k',
    #             label='alpha CV= '+str(model_lars.alpha_))
    # plt.legend()
    
    # plt.xlabel(r'$\alpha$')
    # plt.ylabel('Mean square error')
    # plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
    #           % t_lasso_lars_cv)
    
    
    # # plt.ylim(ymin, ymax)
        
    # # saveFigCustom(plt.gcf, modelName+'-modelSelection_CV-LARS', savePath)
    
    
    # #########################################################################
    
#%% --Subplot all model alpha selections in 1 fig- log X scale
    fig, ax= plt.subplots(3,1, sharex=True)
    
    #-aic/bic
    plt.subplot(3,1,1)
    
    plot_ic_criterion(model_aic, 'AIC', 'b')
    plot_ic_criterion(model_bic, 'BIC', 'r')
    plt.legend()
    plt.title('Information-criterion for model selection (training time %.3fs)'
              % t_bic)
    
    #-LASSO coordinate descent
    plt.subplot(3,1,2)
    plt.semilogx(model_cd.alphas_ + EPSILON, model_cd.mse_path_, ':')
    plt.plot(model_cd.alphas_ + EPSILON, model_cd.mse_path_.mean(axis=-1), 'k',
            label='Average across the folds', linewidth=2)
    plt.axvline(model_cd.alpha_ + EPSILON, linestyle='--', color='k', linewidth=3,
            label='alpha: CV estimate= '+str(model_cd.alpha_))
    
    indAlpha= np.where(model_cd.alphas_==model_cd.alpha_)
    estMSE= model_cd.mse_path_[indAlpha, :].mean()
                 
    plt.axhline(estMSE, linestyle='--', color='blue',
           label='est MSE CV= '+str(estMSE))
    
    plt.legend()
    
    plt.xlabel(r'$\alpha$')
    plt.ylabel('Mean square error')
    plt.title('Mean square error on each fold: coordinate descent '
              '(train time: %.2fs)' % t_lasso_cv)
    plt.axis('tight')
    
    #-LASSO LARS
    plt.subplot(3,1,3, sharey= ax[2]) #share y axis with other MSE plots
    plt.semilogx(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_, ':')
    plt.semilogx(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_.mean(axis=-1), 'k',
                 label='Average across the folds', linewidth=2)
    plt.axvline(model_lars.alpha_+EPSILON, linestyle='--', color='k', linewidth=3,
                label='alpha CV= '+str(model_lars.alpha_))
    
      
    indAlpha= np.where(model_lars.cv_alphas_==model_lars.alpha_)
    estMSE= model_lars.mse_path_[indAlpha, :].mean()
                 
    plt.axhline(estMSE, linestyle='--', color='blue',
           label='est MSE CV= '+str(estMSE))
    
    plt.legend()
    
    plt.xlabel(r'$\alpha$')
    plt.ylabel('Mean square error')
    plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
              % t_lasso_lars_cv)
    
    
    saveFigCustom(plt.gcf, modelName+'-modelSelection_Comparison_logX', savePath)
    
    #%% --Subplot all model alpha selections in 1 fig- normal X scale
    
    fig, ax= plt.subplots(3,1, sharex=True)
    
    #-aic/bic
    plt.subplot(3,1,1)
    
    plot_ic_criterion_nonLog(model_aic, 'AIC', 'b')
    plot_ic_criterion_nonLog(model_bic, 'BIC', 'r')
    plt.legend()
    plt.title('Information-criterion for model selection (training time %.3fs)'
              % t_bic)
    
    #-LASSO coordinate descent
    plt.subplot(3,1,2)
    plt.plot(model_cd.alphas_ + EPSILON, model_cd.mse_path_, ':')
    plt.plot(model_cd.alphas_ + EPSILON, model_cd.mse_path_.mean(axis=-1), 'k',
            label='Average across the folds', linewidth=2)
    plt.axvline(model_cd.alpha_ + EPSILON, linestyle='--', color='k', linewidth=3,
            label='alpha: CV estimate= '+str(model_cd.alpha_))
    
    indAlpha= np.where(model_cd.alphas_==model_cd.alpha_)
    estMSE= model_cd.mse_path_[indAlpha, :].mean()
                 
    plt.axhline(estMSE, linestyle='--', color='blue',
           label='est MSE CV= '+str(estMSE))
    
    plt.legend()
    
    plt.xlabel(r'$\alpha$')
    plt.ylabel('Mean square error')
    plt.title('Mean square error on each fold: coordinate descent '
              '(train time: %.2fs)' % t_lasso_cv)
    plt.axis('tight')
    
    #-LASSO LARS
    plt.subplot(3,1,3, sharey= ax[2]) #share y axis with other MSE plots
    plt.plot(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_, ':')
    plt.plot(model_lars.cv_alphas_ + EPSILON, model_lars.mse_path_.mean(axis=-1), 'k',
                 label='Average across the folds', linewidth=2)
    plt.axvline(model_lars.alpha_ +EPSILON, linestyle='--', color='k', linewidth=3,
                label='alpha CV= '+str(model_lars.alpha_))
    
      
    indAlpha= np.where(model_lars.cv_alphas_==model_lars.alpha_)
    estMSE= model_lars.mse_path_[indAlpha, :].mean()
                 
    plt.axhline(estMSE, linestyle='--', color='blue',
           label='est MSE CV= '+str(estMSE))
    
    plt.legend()
    
    plt.xlabel(r'$\alpha$')
    plt.ylabel('Mean square error')
    plt.title('Mean square error on each fold: Lars (train time: %.2fs)'
              % t_lasso_lars_cv)
    

    saveFigCustom(plt.gcf, modelName+'-modelSelection_Comparison', savePath)

            
    #%% --Return model results
    
    return model_aic, model_bic, model_cd, model_lars, EPSILON

